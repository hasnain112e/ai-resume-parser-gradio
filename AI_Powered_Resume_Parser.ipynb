{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOjKEXAVzs+XigvxnQNfiuR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hasnain112e/ai-resume-parser-gradio/blob/main/AI_Powered_Resume_Parser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d22e3d5a"
      },
      "source": [
        "# Task\n",
        "Create a powerful end-to-end AI-powered Resume Parser Web App that automates resume analysis and extracts essential candidate information using Natural Language Processing (NLP). The application will enable users to upload resumes in PDF format, and through smart parsing logic, it will extract and display key sections such as:\n",
        "\n",
        "Full Name\n",
        "\n",
        "Email Address\n",
        "\n",
        "Phone Number\n",
        "\n",
        "LinkedIn (optional)\n",
        "\n",
        "Education History\n",
        "\n",
        "Technical & Soft Skills\n",
        "\n",
        "Work Experience\n",
        "\n",
        "Certifications (optional)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b86ab0a0"
      },
      "source": [
        "## Set up gradio environment\n",
        "\n",
        "### Subtask:\n",
        "Install the Gradio library and any other necessary libraries for handling file uploads and displaying results within Gradio.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1640515"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to install the required libraries for Gradio, PDF handling, and NLP, and then download the necessary spaCy model as specified in the instructions for this subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b23e388",
        "outputId": "87693257-816a-415d-d9d3-51b9ab15eaa2"
      },
      "source": [
        "%pip install PyMuPDF"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.11/dist-packages (1.26.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3162c470",
        "outputId": "0d75e312-6a61-4f6d-ac78-1fafefb8bc65"
      },
      "source": [
        "%pip install gradio fitz spacy\n",
        "%run -m spacy download en_core_web_sm"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.38.0)\n",
            "Requirement already satisfied: fitz in /usr/local/lib/python3.11/dist-packages (0.0.1.dev2)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.7)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.116.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.6.0)\n",
            "Requirement already satisfied: gradio-client==1.11.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.11.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.33.4)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.11.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.7)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.12.4)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.47.1)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.14.1)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.35.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.11.0->gradio) (2025.7.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.11.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: configobj in /usr/local/lib/python3.11/dist-packages (from fitz) (5.0.9)\n",
            "Requirement already satisfied: configparser in /usr/local/lib/python3.11/dist-packages (from fitz) (7.2.0)\n",
            "Requirement already satisfied: httplib2 in /usr/local/lib/python3.11/dist-packages (from fitz) (0.22.0)\n",
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.11/dist-packages (from fitz) (5.3.2)\n",
            "Requirement already satisfied: nipype in /usr/local/lib/python3.11/dist-packages (from fitz) (1.10.0)\n",
            "Requirement already satisfied: pyxnat in /usr/local/lib/python3.11/dist-packages (from fitz) (1.6.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from fitz) (1.16.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (1.1.5)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.0.post1)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2->fitz) (3.2.3)\n",
            "Requirement already satisfied: importlib-resources>=5.12 in /usr/local/lib/python3.11/dist-packages (from nibabel->fitz) (6.5.2)\n",
            "Requirement already satisfied: networkx>=2.5 in /usr/local/lib/python3.11/dist-packages (from nipype->fitz) (3.5)\n",
            "Requirement already satisfied: prov>=1.5.2 in /usr/local/lib/python3.11/dist-packages (from nipype->fitz) (2.1.1)\n",
            "Requirement already satisfied: pydot>=1.2.3 in /usr/local/lib/python3.11/dist-packages (from nipype->fitz) (3.0.4)\n",
            "Requirement already satisfied: rdflib>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from nipype->fitz) (7.1.4)\n",
            "Requirement already satisfied: simplejson>=3.8.0 in /usr/local/lib/python3.11/dist-packages (from nipype->fitz) (3.20.1)\n",
            "Requirement already satisfied: traits>=6.2 in /usr/local/lib/python3.11/dist-packages (from nipype->fitz) (7.0.2)\n",
            "Requirement already satisfied: acres in /usr/local/lib/python3.11/dist-packages (from nipype->fitz) (0.5.0)\n",
            "Requirement already satisfied: etelemetry>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from nipype->fitz) (0.3.1)\n",
            "Requirement already satisfied: looseversion!=1.2 in /usr/local/lib/python3.11/dist-packages (from nipype->fitz) (1.3.0)\n",
            "Requirement already satisfied: puremagic in /usr/local/lib/python3.11/dist-packages (from nipype->fitz) (1.30)\n",
            "Requirement already satisfied: lxml>=4.3 in /usr/local/lib/python3.11/dist-packages (from pyxnat->fitz) (5.4.0)\n",
            "Requirement already satisfied: pathlib>=1.0 in /usr/local/lib/python3.11/dist-packages (from pyxnat->fitz) (1.0.1)\n",
            "Requirement already satisfied: ci-info>=0.2 in /usr/local/lib/python3.11/dist-packages (from etelemetry>=0.3.1->nipype->fitz) (0.3.0)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8913fd5c"
      },
      "source": [
        "## Adapt the resume parsing logic for gradio\n",
        "\n",
        "### Subtask:\n",
        "Adapt the existing parsing functions to work within the Gradio request-response cycle and handle the uploaded file data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b162c2ff"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the main parsing function `parse_resume_gradio` that takes a file path, reads the PDF content, calls the existing parsing functions, formats the output, and returns it, as required by the instructions. This function will adapt the existing parsing logic to the Gradio request-response cycle.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1e5b2f9f",
        "outputId": "163c96a1-589c-48a9-d627-6820ae81a932"
      },
      "source": [
        "%pip install gradio fitz spacy\n",
        "%run -m spacy download en_core_web_sm"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.38.0)\n",
            "Requirement already satisfied: fitz in /usr/local/lib/python3.11/dist-packages (0.0.1.dev2)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.7)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.116.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.6.0)\n",
            "Requirement already satisfied: gradio-client==1.11.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.11.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.33.4)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.11.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.7)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.12.4)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.47.1)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.14.1)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.35.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.11.0->gradio) (2025.7.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.11.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: configobj in /usr/local/lib/python3.11/dist-packages (from fitz) (5.0.9)\n",
            "Requirement already satisfied: configparser in /usr/local/lib/python3.11/dist-packages (from fitz) (7.2.0)\n",
            "Requirement already satisfied: httplib2 in /usr/local/lib/python3.11/dist-packages (from fitz) (0.22.0)\n",
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.11/dist-packages (from fitz) (5.3.2)\n",
            "Requirement already satisfied: nipype in /usr/local/lib/python3.11/dist-packages (from fitz) (1.10.0)\n",
            "Requirement already satisfied: pyxnat in /usr/local/lib/python3.11/dist-packages (from fitz) (1.6.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from fitz) (1.16.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (1.1.5)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.0.post1)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2->fitz) (3.2.3)\n",
            "Requirement already satisfied: importlib-resources>=5.12 in /usr/local/lib/python3.11/dist-packages (from nibabel->fitz) (6.5.2)\n",
            "Requirement already satisfied: networkx>=2.5 in /usr/local/lib/python3.11/dist-packages (from nipype->fitz) (3.5)\n",
            "Requirement already satisfied: prov>=1.5.2 in /usr/local/lib/python3.11/dist-packages (from nipype->fitz) (2.1.1)\n",
            "Requirement already satisfied: pydot>=1.2.3 in /usr/local/lib/python3.11/dist-packages (from nipype->fitz) (3.0.4)\n",
            "Requirement already satisfied: rdflib>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from nipype->fitz) (7.1.4)\n",
            "Requirement already satisfied: simplejson>=3.8.0 in /usr/local/lib/python3.11/dist-packages (from nipype->fitz) (3.20.1)\n",
            "Requirement already satisfied: traits>=6.2 in /usr/local/lib/python3.11/dist-packages (from nipype->fitz) (7.0.2)\n",
            "Requirement already satisfied: acres in /usr/local/lib/python3.11/dist-packages (from nipype->fitz) (0.5.0)\n",
            "Requirement already satisfied: etelemetry>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from nipype->fitz) (0.3.1)\n",
            "Requirement already satisfied: looseversion!=1.2 in /usr/local/lib/python3.11/dist-packages (from nipype->fitz) (1.3.0)\n",
            "Requirement already satisfied: puremagic in /usr/local/lib/python3.11/dist-packages (from nipype->fitz) (1.30)\n",
            "Requirement already satisfied: lxml>=4.3 in /usr/local/lib/python3.11/dist-packages (from pyxnat->fitz) (5.4.0)\n",
            "Requirement already satisfied: pathlib>=1.0 in /usr/local/lib/python3.11/dist-packages (from pyxnat->fitz) (1.0.1)\n",
            "Requirement already satisfied: ci-info>=0.2 in /usr/local/lib/python3.11/dist-packages (from etelemetry>=0.3.1->nipype->fitz) (0.3.0)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6f9e9ef3",
        "outputId": "cfb3c63b-edbe-4eb9-f37a-3f336dff7faf"
      },
      "source": [
        "import fitz # PyMuPDF\n",
        "import re\n",
        "import spacy\n",
        "import io\n",
        "import os\n",
        "\n",
        "# Load the spaCy model outside of the request handling\n",
        "# This assumes the spaCy model \"en_core_web_sm\" has been downloaded previously\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    print(\"spaCy model loaded successfully.\") # Print to console for verification\n",
        "except Exception as e:\n",
        "    nlp = None\n",
        "    print(f\"Error loading spaCy model: {e}\") # Print error to console\n",
        "\n",
        "# Redefine the parsing functions to ensure they are available in this scope\n",
        "def extract_text_from_pdf(pdf_stream):\n",
        "    \"\"\"Extracts text from a PDF file stream.\"\"\"\n",
        "    text = \"\"\n",
        "    try:\n",
        "        # Use a file-like object directly with fitz.open\n",
        "        doc = fitz.open(stream=pdf_stream.read(), filetype=\"pdf\")\n",
        "        for page_num in range(doc.page_count):\n",
        "            page = doc.load_page(page_num)\n",
        "            text += page.get_text()\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text from PDF: {e}\") # Print error to console\n",
        "        return None\n",
        "    return text\n",
        "\n",
        "def extract_contact_info(text):\n",
        "    \"\"\"Extracts email, phone numbers, and LinkedIn profiles using regex.\"\"\"\n",
        "    email = re.findall(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', text)\n",
        "    phone = re.findall(r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}', text)\n",
        "    # Regex for LinkedIn profile URLs\n",
        "    # This pattern looks for common LinkedIn URL structures\n",
        "    linkedin = re.findall(r'(?:http(s)?:\\/\\/)?([\\w]+\\.)?linkedin\\.com\\/(pub|in|profile)\\/([-a-zA-Z0-9]+)\\/?', text)\n",
        "    # Format the LinkedIn results to get just the URLs or relevant parts\n",
        "    linkedin_urls = []\n",
        "    for match in linkedin:\n",
        "         # Reconstruct the URL parts that were captured\n",
        "        protocol = match[0] if match[0] else '' # http or https\n",
        "        subdomain = match[1] if match[1] else '' # www. or empty\n",
        "        profile_type = match[2] if match[2] else '' # pub, in, or profile\n",
        "        profile_id = match[3] if match[3] else '' # the profile ID\n",
        "        linkedin_urls.append(f\"{'http'+protocol+'://' if protocol or subdomain else ''}{subdomain}linkedin.com/{profile_type}/{profile_id}\")\n",
        "\n",
        "\n",
        "    return {\"email\": email, \"phone\": phone, \"linkedin\": linkedin_urls}\n",
        "\n",
        "\n",
        "def parse_certifications(certifications_text):\n",
        "    \"\"\"Parses the text identified as 'Certifications' to extract individual certifications.\"\"\"\n",
        "    certifications_list = []\n",
        "    if certifications_text:\n",
        "        # Simple parsing: split by common separators like newlines, commas, or semicolons\n",
        "        # This is a basic approach and can be improved with more sophisticated pattern matching\n",
        "        lines = certifications_text.split('\\n')\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                # Further refine parsing based on potential patterns within lines\n",
        "                # For now, just add non-empty lines as individual certifications\n",
        "                certifications_list.append(line)\n",
        "    return certifications_list\n",
        "\n",
        "\n",
        "def extract_sections(text):\n",
        "    \"\"\"Extracts sections like Education, Skills, Work Experience, and Certifications.\"\"\"\n",
        "    sections = {}\n",
        "    # Simple keyword-based extraction (can be improved with more sophisticated NLP)\n",
        "    keywords = {\n",
        "        \"education\": [\"education\", \"academic\"],\n",
        "        \"skills\": [\"skills\", \"proficiencies\"],\n",
        "        \"experience\": [\"experience\", \"work history\", \"employment\"],\n",
        "        # Updated keywords for certifications for potentially better matching\n",
        "        \"certifications\": [\"certifications\", \"licenses\", \"professional development\", \"training\", \"awards\"]\n",
        "    }\n",
        "\n",
        "    text_lower = text.lower()\n",
        "\n",
        "    # Sort keywords by their appearance in the text to better identify section boundaries\n",
        "    found_keywords = sorted([\n",
        "        (text_lower.find(kw), section, kw) for section, kws in keywords.items() for kw in kws if kw in text_lower\n",
        "    ])\n",
        "\n",
        "    # Extract sections based on the order of keywords\n",
        "    for i, (start_index, section, kw) in enumerate(found_keywords):\n",
        "        if start_index != -1:\n",
        "            # Find the end index for the current section\n",
        "            end_index = len(text)\n",
        "            if i + 1 < len(found_keywords):\n",
        "                next_start_index, _, _ = found_keywords[i+1]\n",
        "                end_index = next_start_index\n",
        "\n",
        "            section_text = text[start_index:end_index].strip()\n",
        "\n",
        "            # Remove the keyword itself from the start of the section text\n",
        "            # Find the exact match case-insensitively and remove it\n",
        "            keyword_match = re.search(r'\\b' + re.escape(kw) + r'\\b', section_text, re.IGNORECASE)\n",
        "            if keyword_match:\n",
        "                section_text = section_text[keyword_match.end():].strip()\n",
        "\n",
        "\n",
        "            sections[section] = section_text.strip()\n",
        "\n",
        "    # If a section wasn't found by keyword but other sections were,\n",
        "    # a simple keyword match might still be useful for a fallback\n",
        "    for section, kws in keywords.items():\n",
        "        if section not in sections:\n",
        "             for kw in kws:\n",
        "                start_index = text_lower.find(kw)\n",
        "                if start_index != -1:\n",
        "                    # Simple approach: take text from keyword until the next potential section header or end of text\n",
        "                    remaining_text = text_lower[start_index:]\n",
        "                    end_index = len(remaining_text)\n",
        "                    for other_section, other_kws in keywords.items():\n",
        "                        if other_section != section:\n",
        "                            for other_kw in other_kws:\n",
        "                                other_kw_index = remaining_text.find(other_kw)\n",
        "                                if other_kw_index != -1 and other_kw_index < end_index:\n",
        "                                    end_index = other_kw_index\n",
        "                    section_text = text[start_index:start_index + end_index]\n",
        "\n",
        "                    # Remove the keyword itself from the start of the section text\n",
        "                    keyword_match = re.search(r'\\b' + re.escape(kw) + r'\\b', section_text, re.IGNORECASE)\n",
        "                    if keyword_match:\n",
        "                        section_text = section_text[keyword_match.end():].strip()\n",
        "\n",
        "                    sections[section] = section_text.strip()\n",
        "                    break # Found a keyword for this section\n",
        "\n",
        "\n",
        "    # Parse the certifications text specifically\n",
        "    certifications_text = sections.get(\"certifications\", \"\")\n",
        "    parsed_certifications = parse_certifications(certifications_text)\n",
        "    sections[\"certifications\"] = parsed_certifications # Store as a list of strings\n",
        "\n",
        "    return sections\n",
        "\n",
        "# Need to define extract_name function for parse_resume to work\n",
        "def extract_name(text, nlp):\n",
        "    \"\"\"Extracts a potential name from the text using spaCy.\"\"\"\n",
        "    if nlp:\n",
        "        doc = nlp(text)\n",
        "        names = []\n",
        "        for ent in doc.ents:\n",
        "            # Assuming PERSON entities are names\n",
        "            if ent.label_ == \"PERSON\":\n",
        "                names.append(ent.text)\n",
        "        # Return the longest name found, or None if no names found\n",
        "        if names:\n",
        "            return max(names, key=len)\n",
        "    return None # Return None if nlp model is not loaded or no name is found\n",
        "\n",
        "\n",
        "def parse_resume_gradio(file_path):\n",
        "    \"\"\"\n",
        "    Main parsing function adapted for Gradio.\n",
        "    Takes a file path, extracts text, and parses resume information.\n",
        "    \"\"\"\n",
        "    if file_path is None:\n",
        "        return \"Please upload a PDF file.\", \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"N/A\" # Return empty fields for Gradio output components\n",
        "\n",
        "    try:\n",
        "        with open(file_path, 'rb') as f:\n",
        "            pdf_stream = io.BytesIO(f.read())\n",
        "\n",
        "        text = extract_text_from_pdf(pdf_stream)\n",
        "\n",
        "        if not text:\n",
        "            return \"Could not extract text from PDF.\", \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"N/A\" # Return empty fields\n",
        "\n",
        "        contact_info = extract_contact_info(text)\n",
        "        name = extract_name(text, nlp)\n",
        "        sections = extract_sections(text)\n",
        "\n",
        "        # Format data for Gradio output\n",
        "        full_name = name if name else \"N/A\"\n",
        "        email = \", \".join(contact_info.get(\"email\", [])) if contact_info.get(\"email\") else \"N/A\"\n",
        "        phone = \", \".join(contact_info.get(\"phone\", [])) if contact_info.get(\"phone\") else \"N/A\"\n",
        "        linkedin = \", \".join(contact_info.get(\"linkedin\", [])) if contact_info.get(\"linkedin\") else \"N/A\"\n",
        "        education = sections.get(\"education\", \"N/A\")\n",
        "        skills = sections.get(\"skills\", \"N/A\")\n",
        "        work_experience = sections.get(\"work_experience\", \"N/A\")\n",
        "        certifications = \", \".join(sections.get(\"certifications\", [])) if sections.get(\"certifications\") else \"N/A\"\n",
        "\n",
        "\n",
        "        return \"Parsing successful!\", full_name, email, phone, linkedin, education, skills, work_experience, certifications\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during Gradio parsing: {e}\") # Print error to console\n",
        "        return f\"An error occurred during parsing: {e}\", \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"N/A\" # Return error message and empty fields\n",
        "\n",
        "print(\"Gradio parsing function defined.\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spaCy model loaded successfully.\n",
            "Gradio parsing function defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpTjr2Gds8KZ",
        "outputId": "573228de-529f-47e2-8e29-bf409b455130"
      },
      "source": [
        "import fitz # PyMuPDF\n",
        "import re\n",
        "import spacy\n",
        "import io\n",
        "import os\n",
        "\n",
        "# Load the spaCy model outside of the request handling\n",
        "# This assumes the spaCy model \"en_core_web_sm\" has been downloaded previously\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    print(\"spaCy model loaded successfully.\") # Print to console for verification\n",
        "except Exception as e:\n",
        "    nlp = None\n",
        "    print(f\"Error loading spaCy model: {e}\") # Print error to console\n",
        "\n",
        "# Redefine the parsing functions to ensure they are available in this scope\n",
        "def extract_text_from_pdf(pdf_stream):\n",
        "    \"\"\"Extracts text from a PDF file stream.\"\"\"\n",
        "    text = \"\"\n",
        "    try:\n",
        "        # Use a file-like object directly with fitz.open\n",
        "        doc = fitz.open(stream=pdf_stream.read(), filetype=\"pdf\")\n",
        "        for page_num in range(doc.page_count):\n",
        "            page = doc.load_page(page_num)\n",
        "            text += page.get_text()\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text from PDF: {e}\") # Print error to console\n",
        "        return None\n",
        "    return text\n",
        "\n",
        "def extract_contact_info(text):\n",
        "    \"\"\"Extracts email, phone numbers, and LinkedIn profiles using regex.\"\"\"\n",
        "    email = re.findall(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', text)\n",
        "    phone = re.findall(r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}', text)\n",
        "    # Regex for LinkedIn profile URLs\n",
        "    # This pattern looks for common LinkedIn URL structures\n",
        "    linkedin = re.findall(r'(?:http(s)?:\\/\\/)?([\\w]+\\.)?linkedin\\.com\\/(pub|in|profile)\\/([-a-zA-Z0-9]+)\\/?', text)\n",
        "    # Format the LinkedIn results to get just the URLs or relevant parts\n",
        "    linkedin_urls = []\n",
        "    for match in linkedin:\n",
        "         # Reconstruct the URL parts that were captured\n",
        "        protocol = match[0] if match[0] else '' # http or https\n",
        "        subdomain = match[1] if match[1] else '' # www. or empty\n",
        "        profile_type = match[2] if match[2] else '' # pub, in, or profile\n",
        "        profile_id = match[3] if match[3] else '' # the profile ID\n",
        "        linkedin_urls.append(f\"{'http'+protocol+'://' if protocol or subdomain else ''}{subdomain}linkedin.com/{profile_type}/{profile_id}\")\n",
        "\n",
        "\n",
        "    return {\"email\": email, \"phone\": phone, \"linkedin\": linkedin_urls}\n",
        "\n",
        "\n",
        "def parse_certifications(certifications_text):\n",
        "    \"\"\"Parses the text identified as 'Certifications' to extract individual certifications.\"\"\"\n",
        "    certifications_list = []\n",
        "    if certifications_text:\n",
        "        # Simple parsing: split by common separators like newlines, commas, or semicolons\n",
        "        # This is a basic approach and can be improved with more sophisticated pattern matching\n",
        "        lines = certifications_text.split('\\n')\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                # Further refine parsing based on potential patterns within lines\n",
        "                # For now, just add non-empty lines as individual certifications\n",
        "                certifications_list.append(line)\n",
        "    return certifications_list\n",
        "\n",
        "\n",
        "def extract_sections(text):\n",
        "    \"\"\"Extracts sections like Education, Skills, Work Experience, and Certifications.\"\"\"\n",
        "    sections = {}\n",
        "    # Simple keyword-based extraction (can be improved with more sophisticated NLP)\n",
        "    keywords = {\n",
        "        \"education\": [\"education\", \"academic\"],\n",
        "        \"skills\": [\"skills\", \"proficiencies\"],\n",
        "        \"experience\": [\"experience\", \"work history\", \"employment\"],\n",
        "        # Updated keywords for certifications for potentially better matching\n",
        "        \"certifications\": [\"certifications\", \"licenses\", \"professional development\", \"training\", \"awards\"]\n",
        "    }\n",
        "\n",
        "    text_lower = text.lower()\n",
        "\n",
        "    # Sort keywords by their appearance in the text to better identify section boundaries\n",
        "    found_keywords = sorted([\n",
        "        (text_lower.find(kw), section, kw) for section, kws in keywords.items() for kw in kws if kw in text_lower\n",
        "    ])\n",
        "\n",
        "    # Extract sections based on the order of keywords\n",
        "    for i, (start_index, section, kw) in enumerate(found_keywords):\n",
        "        if start_index != -1:\n",
        "            # Find the end index for the current section\n",
        "            end_index = len(text)\n",
        "            if i + 1 < len(found_keywords):\n",
        "                next_start_index, _, _ = found_keywords[i+1]\n",
        "                end_index = next_start_index\n",
        "\n",
        "            section_text = text[start_index:end_index].strip()\n",
        "\n",
        "            # Remove the keyword itself from the start of the section text\n",
        "            # Find the exact match case-insensitively and remove it\n",
        "            keyword_match = re.search(r'\\b' + re.escape(kw) + r'\\b', section_text, re.IGNORECASE)\n",
        "            if keyword_match:\n",
        "                section_text = section_text[keyword_match.end():].strip()\n",
        "\n",
        "\n",
        "            sections[section] = section_text.strip()\n",
        "\n",
        "    # If a section wasn't found by keyword but other sections were,\n",
        "    # a simple keyword match might still be useful for a fallback\n",
        "    for section, kws in keywords.items():\n",
        "        if section not in sections:\n",
        "             for kw in kws:\n",
        "                start_index = text_lower.find(kw)\n",
        "                if start_index != -1:\n",
        "                    # Simple approach: take text from keyword until the next potential section header or end of text\n",
        "                    remaining_text = text_lower[start_index:]\n",
        "                    end_index = len(remaining_text)\n",
        "                    for other_section, other_kws in keywords.items():\n",
        "                        if other_section != section:\n",
        "                            for other_kw in other_kws:\n",
        "                                other_kw_index = remaining_text.find(other_kw)\n",
        "                                if other_kw_index != -1 and other_kw_index < end_index:\n",
        "                                    end_index = other_kw_index\n",
        "                    section_text = text[start_index:start_index + end_index]\n",
        "\n",
        "                    # Remove the keyword itself from the start of the section text\n",
        "                    keyword_match = re.search(r'\\b' + re.escape(kw) + r'\\b', section_text, re.IGNORECASE)\n",
        "                    if keyword_match:\n",
        "                        section_text = section_text[keyword_match.end():].strip()\n",
        "\n",
        "                    sections[section] = section_text.strip()\n",
        "                    break # Found a keyword for this section\n",
        "\n",
        "\n",
        "    # Parse the certifications text specifically\n",
        "    certifications_text = sections.get(\"certifications\", \"\")\n",
        "    parsed_certifications = parse_certifications(certifications_text)\n",
        "    sections[\"certifications\"] = parsed_certifications # Store as a list of strings\n",
        "\n",
        "    return sections\n",
        "\n",
        "# Need to define extract_name function for parse_resume to work\n",
        "def extract_name(text, nlp):\n",
        "    \"\"\"Extracts a potential name from the text using spaCy.\"\"\"\n",
        "    if nlp:\n",
        "        doc = nlp(text)\n",
        "        names = []\n",
        "        for ent in doc.ents:\n",
        "            # Assuming PERSON entities are names\n",
        "            if ent.label_ == \"PERSON\":\n",
        "                names.append(ent.text)\n",
        "        # Return the longest name found, or None if no names found\n",
        "        if names:\n",
        "            return max(names, key=len)\n",
        "    return None # Return None if nlp model is not loaded or no name is found\n",
        "\n",
        "\n",
        "def parse_resume_gradio(file_path):\n",
        "    \"\"\"\n",
        "    Main parsing function adapted for Gradio.\n",
        "    Takes a file path, extracts text, and parses resume information.\n",
        "    \"\"\"\n",
        "    if file_path is None:\n",
        "        return \"Please upload a PDF file.\", \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"N/A\" # Return empty fields for Gradio output components\n",
        "\n",
        "    try:\n",
        "        with open(file_path, 'rb') as f:\n",
        "            pdf_stream = io.BytesIO(f.read())\n",
        "\n",
        "        text = extract_text_from_pdf(pdf_stream)\n",
        "\n",
        "        if not text:\n",
        "            return \"Could not extract text from PDF.\", \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"N/A\" # Return empty fields\n",
        "\n",
        "        contact_info = extract_contact_info(text)\n",
        "        name = extract_name(text, nlp)\n",
        "        sections = extract_sections(text)\n",
        "\n",
        "        # Format data for Gradio output\n",
        "        full_name = name if name else \"N/A\"\n",
        "        email = \", \".join(contact_info.get(\"email\", [])) if contact_info.get(\"email\") else \"N/A\"\n",
        "        phone = \", \".join(contact_info.get(\"phone\", [])) if contact_info.get(\"phone\") else \"N/A\"\n",
        "        linkedin = \", \".join(contact_info.get(\"linkedin\", [])) if contact_info.get(\"linkedin\") else \"N/A\"\n",
        "        education = sections.get(\"education\", \"N/A\")\n",
        "        skills = sections.get(\"skills\", \"N/A\")\n",
        "        work_experience = sections.get(\"work_experience\", \"N/A\")\n",
        "        certifications = \", \".join(sections.get(\"certifications\", [])) if sections.get(\"certifications\") else \"N/A\"\n",
        "\n",
        "\n",
        "        return \"Parsing successful!\", full_name, email, phone, linkedin, education, skills, work_experience, certifications\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during Gradio parsing: {e}\") # Print error to console\n",
        "        return f\"An error occurred during parsing: {e}\", \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"N/A\" # Return error message and empty fields\n",
        "\n",
        "print(\"Gradio parsing function defined.\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spaCy model loaded successfully.\n",
            "Gradio parsing function defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "860cd5ea"
      },
      "source": [
        "## Build the gradio interface\n",
        "\n",
        "### Subtask:\n",
        "Design the user interface using Gradio components, including a file upload component and various components to display the extracted resume information (e.g., text boxes, lists).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9e922f9"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the Gradio input and output components as specified in the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3230ce07",
        "outputId": "6742e2a5-0b34-4ecd-ad5c-13d2a9bdf5cc"
      },
      "source": [
        "import gradio as gr\n",
        "\n",
        "# Define the input component (File Upload)\n",
        "resume_file_input = gr.File(\n",
        "    label=\"Upload your Resume (PDF only)\",\n",
        "    file_count=\"single\",\n",
        "    file_types=[\".pdf\"]\n",
        ")\n",
        "\n",
        "# Define the output components for extracted information\n",
        "output_full_name = gr.Textbox(label=\"Full Name\")\n",
        "output_email = gr.Textbox(label=\"Email\")\n",
        "output_phone = gr.Textbox(label=\"Phone Number\")\n",
        "output_linkedin = gr.Textbox(label=\"LinkedIn\")\n",
        "output_education = gr.Textbox(label=\"Education History\")\n",
        "output_skills = gr.Textbox(label=\"Technical & Soft Skills\")\n",
        "output_work_experience = gr.Textbox(label=\"Work Experience\")\n",
        "output_certifications = gr.Textbox(label=\"Certifications\")\n",
        "output_status = gr.Textbox(label=\"Parsing Status\") # Add a status message output\n",
        "\n",
        "# Create a list of the output components\n",
        "output_components = [\n",
        "    output_status,\n",
        "    output_full_name,\n",
        "    output_email,\n",
        "    output_phone,\n",
        "    output_linkedin,\n",
        "    output_education,\n",
        "    output_skills,\n",
        "    output_work_experience,\n",
        "    output_certifications,\n",
        "]\n",
        "\n",
        "print(\"Gradio input and output components defined.\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradio input and output components defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc183cd9"
      },
      "source": [
        "## Integrate parsing logic with the gradio interface\n",
        "\n",
        "### Subtask:\n",
        "Connect the file upload component to the parsing functions and link the output of the parsing functions to the display components in the Gradio interface.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7054168d"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to define the `parse_resume_gradio` function, the input and output Gradio components, create the Gradio interface using `gr.Interface`, and launch the interface, as per the subtask instructions. I will combine these steps into a single code block since they are sequentially dependent on each other and directly contribute to building and launching the Gradio app. I will ensure all necessary parsing functions are included or accessible in this scope.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 633
        },
        "id": "0ba7ee19",
        "outputId": "a31ce199-1876-4de1-f6e2-078b4cb2818d"
      },
      "source": [
        "import gradio as gr\n",
        "import fitz # PyMuPDF\n",
        "import re\n",
        "import spacy\n",
        "import io\n",
        "import os\n",
        "\n",
        "# Load the spaCy model outside of the request handling\n",
        "# This assumes the spaCy model \"en_core_web_sm\" has been downloaded previously\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    print(\"spaCy model loaded successfully.\") # Print to console for verification\n",
        "except Exception as e:\n",
        "    nlp = None\n",
        "    print(f\"Error loading spaCy model: {e}\") # Print error to console\n",
        "\n",
        "# Redefine the parsing functions to ensure they are available in this scope\n",
        "def extract_text_from_pdf(pdf_stream):\n",
        "    \"\"\"Extracts text from a PDF file stream.\"\"\"\n",
        "    text = \"\"\n",
        "    try:\n",
        "        # Use a file-like object directly with fitz.open\n",
        "        doc = fitz.open(stream=pdf_stream.read(), filetype=\"pdf\")\n",
        "        for page_num in range(doc.page_count):\n",
        "            page = doc.load_page(page_num)\n",
        "            text += page.get_text()\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text from PDF: {e}\") # Print error to console\n",
        "        return None\n",
        "    return text\n",
        "\n",
        "def extract_contact_info(text):\n",
        "    \"\"\"Extracts email, phone numbers, and LinkedIn profiles using regex.\"\"\"\n",
        "    email = re.findall(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', text)\n",
        "    phone = re.findall(r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}', text)\n",
        "    # Regex for LinkedIn profile URLs\n",
        "    # This pattern looks for common LinkedIn URL structures\n",
        "    linkedin = re.findall(r'(?:http(s)?:\\/\\/)?([\\w]+\\.)?linkedin\\.com\\/(pub|in|profile)\\/([-a-zA-Z0-9]+)\\/?', text)\n",
        "    # Format the LinkedIn results to get just the URLs or relevant parts\n",
        "    linkedin_urls = []\n",
        "    for match in linkedin:\n",
        "         # Reconstruct the URL parts that were captured\n",
        "        protocol = match[0] if match[0] else '' # http or https\n",
        "        subdomain = match[1] if match[1] else '' # www. or empty\n",
        "        profile_type = match[2] if match[2] else '' # pub, in or profile\n",
        "        profile_id = match[3] if match[3] else '' # the profile ID\n",
        "        linkedin_urls.append(f\"{'http'+protocol+'://' if protocol or subdomain else ''}{subdomain}linkedin.com/{profile_type}/{profile_id}\")\n",
        "\n",
        "\n",
        "    return {\"email\": email, \"phone\": phone, \"linkedin\": linkedin_urls}\n",
        "\n",
        "\n",
        "def parse_certifications(certifications_text):\n",
        "    \"\"\"Parses the text identified as 'Certifications' to extract individual certifications.\"\"\"\n",
        "    certifications_list = []\n",
        "    if certifications_text:\n",
        "        # Simple parsing: split by common separators like newlines, commas, or semicolons\n",
        "        # This is a basic approach and can be improved with more sophisticated pattern matching\n",
        "        lines = certifications_text.split('\\n')\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                # Further refine parsing based on potential patterns within lines\n",
        "                # For now, just add non-empty lines as individual certifications\n",
        "                certifications_list.append(line)\n",
        "    return certifications_list\n",
        "\n",
        "\n",
        "def extract_sections(text):\n",
        "    \"\"\"Extracts sections like Education, Skills, Work Experience, and Certifications.\"\"\"\n",
        "    sections = {}\n",
        "    # Simple keyword-based extraction (can be improved with more sophisticated NLP)\n",
        "    keywords = {\n",
        "        \"education\": [\"education\", \"academic\"],\n",
        "        \"skills\": [\"skills\", \"proficiencies\"],\n",
        "        \"experience\": [\"experience\", \"work history\", \"employment\"],\n",
        "        # Updated keywords for certifications for potentially better matching\n",
        "        \"certifications\": [\"certifications\", \"licenses\", \"professional development\", \"training\", \"awards\"]\n",
        "    }\n",
        "\n",
        "    text_lower = text.lower()\n",
        "\n",
        "    # Sort keywords by their appearance in the text to better identify section boundaries\n",
        "    found_keywords = sorted([\n",
        "        (text_lower.find(kw), section, kw) for section, kws in keywords.items() for kw in kws if kw in text_lower\n",
        "    ])\n",
        "\n",
        "    # Extract sections based on the order of keywords\n",
        "    for i, (start_index, section, kw) in enumerate(found_keywords):\n",
        "        if start_index != -1:\n",
        "            # Find the end index for the current section\n",
        "            end_index = len(text)\n",
        "            if i + 1 < len(found_keywords):\n",
        "                next_start_index, _, _ = found_keywords[i+1]\n",
        "                end_index = next_start_index\n",
        "\n",
        "            section_text = text[start_index:end_index].strip()\n",
        "\n",
        "            # Remove the keyword itself from the start of the section text\n",
        "            # Find the exact match case-insensitively and remove it\n",
        "            keyword_match = re.search(r'\\b' + re.escape(kw) + r'\\b', section_text, re.IGNORECASE)\n",
        "            if keyword_match:\n",
        "                section_text = section_text[keyword_match.end():].strip()\n",
        "\n",
        "\n",
        "            sections[section] = section_text.strip()\n",
        "\n",
        "    # If a section wasn't found by keyword but other sections were,\n",
        "    # a simple keyword match might still be useful for a fallback\n",
        "    for section, kws in keywords.items():\n",
        "        if section not in sections:\n",
        "             for kw in kws:\n",
        "                start_index = text_lower.find(kw)\n",
        "                if start_index != -1:\n",
        "                    # Simple approach: take text from keyword until the next potential section header or end of text\n",
        "                    remaining_text = text_lower[start_index:]\n",
        "                    end_index = len(remaining_text)\n",
        "                    for other_section, other_kws in keywords.items():\n",
        "                        if other_section != section:\n",
        "                            for other_kw in other_kws:\n",
        "                                other_kw_index = remaining_text.find(other_kw)\n",
        "                                if other_kw_index != -1 and other_kw_index < end_index:\n",
        "                                    end_index = other_kw_index\n",
        "                    section_text = text[start_index:start_index + end_index]\n",
        "\n",
        "                    # Remove the keyword itself from the start of the section text\n",
        "                    keyword_match = re.search(r'\\b' + re.escape(kw) + r'\\b', section_text, re.IGNORECASE)\n",
        "                    if keyword_match:\n",
        "                        section_text = section_text[keyword_match.end():].strip()\n",
        "\n",
        "                    sections[section] = section_text.strip()\n",
        "                    break # Found a keyword for this section\n",
        "\n",
        "\n",
        "    # Parse the certifications text specifically\n",
        "    certifications_text = sections.get(\"certifications\", \"\")\n",
        "    parsed_certifications = parse_certifications(certifications_text)\n",
        "    sections[\"certifications\"] = parsed_certifications # Store as a list of strings\n",
        "\n",
        "    return sections\n",
        "\n",
        "# Need to define extract_name function for parse_resume to work\n",
        "def extract_name(text, nlp):\n",
        "    \"\"\"Extracts a potential name from the text using spaCy.\"\"\"\n",
        "    if nlp:\n",
        "        doc = nlp(text)\n",
        "        names = []\n",
        "        for ent in doc.ents:\n",
        "            # Assuming PERSON entities are names\n",
        "            if ent.label_ == \"PERSON\":\n",
        "                names.append(ent.text)\n",
        "        # Return the longest name found, or None if no names found\n",
        "        if names:\n",
        "            return max(names, key=len)\n",
        "    return None # Return None if nlp model is not loaded or no name is found\n",
        "\n",
        "\n",
        "def parse_resume_gradio(file_path):\n",
        "    \"\"\"\n",
        "    Main parsing function adapted for Gradio.\n",
        "    Takes a file path, extracts text, and parses resume information.\n",
        "    Returns a tuple of parsed data for Gradio output components.\n",
        "    \"\"\"\n",
        "    # Initialize all output fields to N/A or empty strings\n",
        "    default_output = (\"Please upload a PDF file.\", \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"N/A\")\n",
        "\n",
        "    if file_path is None:\n",
        "        return default_output\n",
        "\n",
        "    try:\n",
        "        with open(file_path, 'rb') as f:\n",
        "            pdf_stream = io.BytesIO(f.read())\n",
        "\n",
        "        text = extract_text_from_pdf(pdf_stream)\n",
        "\n",
        "        if not text:\n",
        "            return \"Could not extract text from PDF.\", \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"N/A\"\n",
        "\n",
        "        # Check if nlp model is loaded before proceeding with name extraction\n",
        "        if nlp is None:\n",
        "             return \"NLP model not loaded. Cannot extract name and potentially other info accurately.\", \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"N/A\"\n",
        "\n",
        "\n",
        "        contact_info = extract_contact_info(text)\n",
        "        name = extract_name(text, nlp) # Use the nlp model here\n",
        "        sections = extract_sections(text)\n",
        "\n",
        "        # Format data for Gradio output\n",
        "        full_name = name if name else \"N/A\"\n",
        "        email = \", \".join(contact_info.get(\"email\", [])) if contact_info.get(\"email\") else \"N/A\"\n",
        "        phone = \", \".join(contact_info.get(\"phone\", [])) if contact_info.get(\"phone\") else \"N/A\"\n",
        "        linkedin = \", \".join(contact_info.get(\"linkedin\", [])) if contact_info.get(\"linkedin\") else \"N/A\"\n",
        "        education = sections.get(\"education\", \"N/A\")\n",
        "        skills = sections.get(\"skills\", \"N/A\")\n",
        "        work_experience = sections.get(\"work_experience\", \"N/A\")\n",
        "        certifications = \", \".join(sections.get(\"certifications\", [])) if sections.get(\"certifications\") else \"N/A\"\n",
        "\n",
        "\n",
        "        return \"Parsing successful!\", full_name, email, phone, linkedin, education, skills, work_experience, certifications\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during Gradio parsing: {e}\") # Print error to console\n",
        "        return f\"An error occurred during parsing: {e}\", \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"N/A\" # Return error message and empty fields\n",
        "\n",
        "\n",
        "# Define the input component (File Upload)\n",
        "resume_file_input = gr.File(\n",
        "    label=\"Upload your Resume (PDF only)\",\n",
        "    file_count=\"single\",\n",
        "    file_types=[\".pdf\"]\n",
        ")\n",
        "\n",
        "# Define the output components for extracted information\n",
        "output_status = gr.Textbox(label=\"Parsing Status\") # Status message output\n",
        "output_full_name = gr.Textbox(label=\"Full Name\")\n",
        "output_email = gr.Textbox(label=\"Email\")\n",
        "output_phone = gr.Textbox(label=\"Phone Number\")\n",
        "output_linkedin = gr.Textbox(label=\"LinkedIn\")\n",
        "output_education = gr.Textbox(label=\"Education History\")\n",
        "output_skills = gr.Textbox(label=\"Technical & Soft Skills\")\n",
        "output_work_experience = gr.Textbox(label=\"Work Experience\")\n",
        "output_certifications = gr.Textbox(label=\"Certifications\")\n",
        "\n",
        "\n",
        "# Create a list of the output components in the order returned by parse_resume_gradio\n",
        "output_components = [\n",
        "    output_status,\n",
        "    output_full_name,\n",
        "    output_email,\n",
        "    output_phone,\n",
        "    output_linkedin,\n",
        "    output_education,\n",
        "    output_skills,\n",
        "    output_work_experience,\n",
        "    output_certifications,\n",
        "]\n",
        "\n",
        "\n",
        "# Create the Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=parse_resume_gradio,\n",
        "    inputs=resume_file_input,\n",
        "    outputs=output_components,\n",
        "    title=\"AI-Powered Resume Parser\",\n",
        "    description=\"Upload a PDF resume to extract key information.\"\n",
        ")\n",
        "\n",
        "print(\"Launching Gradio interface...\")\n",
        "\n",
        "# Launch the Gradio interface\n",
        "# Setting share=True will create a public shareable link (useful for testing/demo)\n",
        "# Setting share=False will only run locally\n",
        "iface.launch(share=False)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spaCy model loaded successfully.\n",
            "Launching Gradio interface...\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n",
            "* To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "                        if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "                            return;\n",
              "                        }\n",
              "                        element.appendChild(document.createTextNode(''));\n",
              "                        const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "\n",
              "                        const external_link = document.createElement('div');\n",
              "                        external_link.innerHTML = `\n",
              "                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n",
              "                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n",
              "                                    https://localhost:${port}${path}\n",
              "                                </a>\n",
              "                            </div>\n",
              "                        `;\n",
              "                        element.appendChild(external_link);\n",
              "\n",
              "                        const iframe = document.createElement('iframe');\n",
              "                        iframe.src = new URL(path, url).toString();\n",
              "                        iframe.height = height;\n",
              "                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n",
              "                        iframe.width = width;\n",
              "                        iframe.style.border = 0;\n",
              "                        element.appendChild(iframe);\n",
              "                    })(7863, \"/\", \"100%\", 500, false, window.element)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bda86c9"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The development process successfully set up the necessary environment by installing `gradio`, `fitz`, and `spacy`, including downloading the `en_core_web_sm` NLP model.\n",
        "*   A critical `ModuleNotFoundError` related to the `fitz` library was encountered during the attempt to adapt the parsing logic for Gradio, preventing PDF text extraction.\n",
        "*   The `ModuleNotFoundError` was resolved by forcefully reinstalling `PyMuPDF`.\n",
        "*   The parsing logic, including functions for extracting text, contact info, sections, and name, was successfully defined and adapted to work with file paths provided by Gradio.\n",
        "*   Gradio components for file upload and displaying parsed data (name, email, phone, LinkedIn, education, skills, experience, certifications, and status) were correctly defined.\n",
        "*   The Gradio interface was successfully created and launched locally, integrating the file input, the parsing function, and the output components.\n",
        "*   The application is ready for manual testing to verify the parsing accuracy across various resume formats.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   Thorough manual testing with a diverse set of resume formats is crucial to evaluate the accuracy and robustness of the keyword-based section extraction and regex patterns.\n",
        "*   Further refinement of the parsing logic, potentially using more advanced NLP techniques (beyond simple spaCy entity recognition and keyword matching) or machine learning models, could significantly improve the accuracy of extracted information, especially for unstructured sections like skills and work experience.\n"
      ]
    }
  ]
}